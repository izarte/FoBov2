0.1.0 -> 
    -> reward: Only given when human is in pixel, and -5 if human was lost
        - distance: exponetial to 2 with k = 0.2 with logic for closest distance, 5 when in center
        - pixel: same applied to pixel to 1 with k = 0.2, error with closest logic, 5 when in center
    -> terminated: Track with 500 steps, stop when standard deviation < 0.01 and loops > 3

0.1.1 -> 
    -> reward: Only given when human was seen in memory, -5 if lost in current and memory, to avoid fake losts
        - distance: same as 0.1.0
        - pixel: no closest logic applied and value up to 2 with k = 6
    -> terminated: Track with 1000 steps, stop with same std deviation as 0.1.0 and abs(loops) > 3
0.1.2 ->
    -> reward: max 3 for human pixel, so distance based goal should be more relevant, and punish reduced to -1 to avoid going to wall so 0 > -5
0.1.3 ->
    -> reward: no negative reward for loosing human in pixel
        - distance: exponetial to 3

0.2.0 ->
    -> Initial robot pose is at 2.5 +- 0.5 and orientated to human with a +-45. Now human moves. Reduced max steps per instance to 3000
0.2.1 ->
    -> Back to -1 reward when human was lost and increase penalty for collision to -10000
0.3.0 ->
    -> Add obstacles to world, random number from 2 to 10. An obstacle can't be between human and its target point